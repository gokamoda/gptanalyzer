<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DissectingGPT Blog</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css"
        integrity="sha512-SzlrxWUlpfuzQ+pcUCosxcglQRNAq/DZjVsC0lE40xsADsfeQoEypE+enwcOiGjk/bSuGGKHEyjSoQ1zVisanQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <script src="https://code.jquery.com/jquery-3.7.0.min.js"></script>
    <script type="text/javascript" src="js/mathjax_config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="style.css">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-DDQCS7FXG9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        
        gtag('config', 'G-DDQCS7FXG9');
    </script>
</head>


<body>

    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="row mt-3">
                    <h2>Linearilty of LayerNormalization</h2>

                    <p>
                        LayerNormalization is defined by the following equation:
                        \[
                        \begin{equation}
                        \text{LayerNorm}(\bm{x}) = \frac{\bm{x} - \mu(\bm{x})\bm{1}}{s(\bm{x})}\odot \bm{\gamma} +
                        \bm{\beta}
                        \label{eq:lm_f}
                        \end{equation}
                        \]
                        Here,
                        \(\mu(\bm{x})\) and \(s(\bm{x})\) is the element-wise mean and the standard deviation of
                        \(\bm{x} \in \mathbb{R}^{d}\), respectively,
                        \(\bm{1}\) is a vector of ones with the same shape as \(\bm{x}\),
                        \(\bm{\gamma}, \bm{\beta} \in \mathbb{R}^{d}\) are learnable parameters,
                        and \(\odot\) represents element-wise multiplication.
                    </p>

                    <p>
                        Now,
                        \[
                        \begin{align}
                        \mu(\bm{x})\bm{1} &= \left(\frac{1}{d}\sum_k\bm{x}^{(k)}\right)\bm{1}\\
                        &=\left(\frac{1}{d}\bm{x}\cdot\bm{1}\right)\bm{1}\\
                        &=\left(\frac{1}{d}\bm{x}\bm{1}^\top\right)\bm{1}\\
                        &=\bm{x}\left(\frac{1}{d}\bm{1}^\top\bm{1}\right)
                        \end{align}
                        \]
                        with \(\bm{x}^{(k)}\) representing the \(k\)-th element of \(\bm{x}\).
                    </p>

                    <p>
                        Also, element-wise multiplication of \(\bm{\gamma}\) can be expressed as matrix multiplication of \(\text{diag}(\bm{\gamma})\).
                        Therefore, LayerNorm can be rewritten as:
                        \[
                        \begin{align}
                        \text{LayerNorm}(\bm{x}) &= \frac{1}{s(\bm{x})}\left(\bm{x} - \mu(\bm{x})\right)\odot \bm{\gamma} + \bm{\beta}\\
                        &= \frac{\bm{x}}{s(\bm{x})}\left(I - \frac{1}{d}\bm{1}^\top\bm{1}\right)\text{diag}(\bm{\gamma}) + \bm{\beta}\\
                        \end{align}
                        \]
                        The only non-linear operation in LayerNorm is the division by \(s(\bm{x})\).
                    </p>

                    <p>
                        By the way, \(I - \frac{1}{d}\bm{1}^\top\bm{1}\) is called the 
                        <a href="https://en.wikipedia.org/wiki/Centering_matrix">centering matrix</a>.
                    </p>


                </div>
                <!-- <div class="row">
                    <div class="col-12">
                        <h2>References</h2>
                        <ol>
                            <li id="lad2023remarkable">
                                <a href="https://arxiv.org/abs/2406.19384">
                                    Lad et al. 2023, The Remarkable Robustness of LLMs: Stages of Inference?
                                </a>

                            </li>
                        </ol>
                    </div>
                </div> -->

            </div>
        </div>
    </div>

    <script>
        $(function () {
            $.get('header.html', function (data) {
                $('body').prepend(data);
            });
            $.get('footer.html', function (data) {
                $('body').append(data);
            });
        });
    </script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz"
        crossorigin="anonymous"></script>
</body>

</html>