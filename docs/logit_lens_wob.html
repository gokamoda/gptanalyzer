<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DissectingGPT Blog</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css"
        integrity="sha512-SzlrxWUlpfuzQ+pcUCosxcglQRNAq/DZjVsC0lE40xsADsfeQoEypE+enwcOiGjk/bSuGGKHEyjSoQ1zVisanQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <script src="https://code.jquery.com/jquery-3.7.0.min.js"></script>
    <script type="text/javascript" src="js/mathjax_config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="style.css">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-DDQCS7FXG9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        
        gtag('config', 'G-DDQCS7FXG9');
    </script>
</head>


<body>

    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="row mt-3">
                    <h2>LogitLens without bias (GPT2)</h2>
                    <p>
                        <a href="logit_lens_wob.html#nostalgibraist2020logit">LogitLens</a>
                         applies \(\text{lm_head}\) to the hidden states \((\bm{h} \in \mathbb{R}^{1\times
                        d})\) of the transformer model. <br>
                        \[
                        \begin{equation}
                        \text{lm_head}(\bm{h}) = \text{ln_f}(\bm{h})\bm{W} \label{eq:lm_head}
                        \end{equation}
                        \]

                        \[
                        \begin{equation}
                        \text{ln_f}(\bm{h}) = \frac{\bm{h} - \mu(\bm{h})\bm{1}}{s(\bm{h})}\odot \bm{\gamma} + \bm{\beta}
                        \label{eq:lm_f}
                        \end{equation}
                        \]

                        Here, \(\bm{W} \in \mathbb{R}^{d\times v}\) is the unembedding matrix,
                        \(\mu(\bm{h})\) and \(s(\bm{h})\) is the element-wise mean and the standard deviation of \(\bm{h}\), respectively,
                        <!-- \(s(\bm{h})\) is the standard deviation of \(\bm{h}\) for each element, -->
                        \(\bm{\gamma}, \bm{\beta} \in \mathbb{R}^{1\times d}\) are learnable parameters,
                        and \(\odot\) represents element-wise multiplication.
                    </p>

                    <p>
                        With LogitLens, one can project the hidden states after each transformer layers to the
                        vocabulary space.<br>
                    </p>
                    <p class="text-center">
                        <img src="img/logit_lens_gpt2.png" alt="LogitLensExample" width="70%">
                        <!-- caption -->
                    <p class="text-center">Example of LogitLens.
                    </p>

                    <p>
                        By combining Equation\eqref{eq:lm_head} and \eqref{eq:lm_f}, we get a bias term for the projection to
                        vocabulary space.

                        \[
                        \begin{align}
                        \text{LogitLens}(\bm{h})
                        &= \left(\frac{\bm{h} - \mu(\bm{h})\bm{1}}{s(\bm{h})}\odot \bm{\gamma} +
                        \bm{\beta}\right)\bm{W}\\
                        &= \left(\frac{\bm{h} - \mu(\bm{h})\bm{1}}{s(\bm{h})}\odot \bm{\gamma}\right)\bm{W} +
                        \bm{\beta}\bm{W} \label{eq:lm_head_bias}
                        \end{align}
                        \]
                    </p>
                    <p>
                        The second term in Equation\eqref{eq:lm_head_bias} is the bias term, which is added to the result of
                        LogitLens regardless of the input. <br>
                        Adding such bias is not reasonable when analyzing "what the model's intermediate states
                        represent".<br>
                        (In GPT2, <a href="logit_lens_wob.html#kobayashi2023transformer">Kobayashi et al. (2023)</a>
                        reports that word frequency in the training corpus is encoded in this bias.)<br>
                        By removing the bias term, we get the following result.
                        <!-- <a href="#belrose2023eliciting">Belrose et al. (2023)</a> also reports that LogitLens is a "biased estimator of the modelâ€™s final output: it systematically puts more probability mass on certain vocabulary items than the final layer does". -->
                    </p>

                    <div class="row row-cols-1 row-cols-md-2 g-0 g-md-3">
                        <div class="col">
                            <img src="img/logit_lens_gpt2.png" alt="LogitLensExample" width="100%">
                        </div>
                        <div class="col">
                            <img src="img/logit_lens_nobias_gpt2.png" alt="LogitLensExample" width="100%">
                        </div>
                    </div>

                    <div class="row row-cols-1 row-cols-md-2 g-0 g-md-3">
                        <div class="col">
                            <img src="img/logit_lens_opt.png" alt="LogitLensExample" width="100%">
                        </div>
                        <div class="col">
                            <img src="img/logit_lens_nobias_opt.png" alt="LogitLensExample" width="100%">
                        </div>
                    </div>




                </div>
                <div class="row">
                    <div class="col-12">
                        <h2>References</h2>
                        <ol>
                            <li id="nostalgibraist2020logit">
                                <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">
                                    Nostalgibraist 2020, interpreting GPT: the logit lens.
                                </a>
                                
                            </li>
                            <li id="kobayashi2023transformer">
                                <a href="https://aclanthology.org/2023.findings-acl.276/">Kobayashi et al. 2023, Transformer Language Models Handle Word Frequency in Prediction
                                    Head.</a>
                                
                            </li>
                            <!-- <li id="belrose2023eliciting">
                                Belrose et al. 2023, Eliciting Latent Predictions from Transformers with the Tuned Lens
                            </li> -->
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        $(function () {
            $.get('header.html', function (data) {
                $('body').prepend(data);
            });
            $.get('footer.html', function (data) {
                $('body').append(data);
            });
        });
    </script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz"
        crossorigin="anonymous"></script>
</body>

</html>